# Master's Thesis
Deploying inference models in production come with a number of challenges:

* Handling multiple machine learning models in a consistent manner. 
* Updating running models with new models. 
* Scaling models appropriately with constraints. 
* Monitoring models in production.

The goal of this project is to research an important question which falls within the domain of autoscaling. 

**Are making autoscaling decisions based on GPU utilization metrics more efficient than making decisions based on the number of inflight requests?**  



















